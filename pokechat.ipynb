{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-21T03:57:10.294791Z",
     "iopub.status.busy": "2024-02-21T03:57:10.294174Z",
     "iopub.status.idle": "2024-02-21T03:58:26.385786Z",
     "shell.execute_reply": "2024-02-21T03:58:26.384645Z",
     "shell.execute_reply.started": "2024-02-21T03:57:10.294755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\n",
      "google-cloud-pubsublite 1.8.3 requires overrides<7.0.0,>=6.0.1, but you have overrides 7.7.0 which is incompatible.\n",
      "jupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "kfp 2.0.1 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "opentelemetry-exporter-otlp 1.19.0 requires opentelemetry-exporter-otlp-proto-grpc==1.19.0, but you have opentelemetry-exporter-otlp-proto-grpc 1.22.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-exporter-otlp-proto-common==1.19.0, but you have opentelemetry-exporter-otlp-proto-common 1.22.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-proto==1.19.0, but you have opentelemetry-proto 1.22.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.19.0 requires opentelemetry-sdk~=1.19.0, but you have opentelemetry-sdk 1.22.0 which is incompatible.\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    transformers \\\n",
    "    accelerate \\\n",
    "    bitsandbytes \\\n",
    "    langchain==0.0.354 \\\n",
    "    chromadb \\\n",
    "    voyageai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T03:58:31.543835Z",
     "iopub.status.busy": "2024-02-21T03:58:31.543143Z",
     "iopub.status.idle": "2024-02-21T03:58:51.275242Z",
     "shell.execute_reply": "2024-02-21T03:58:51.274041Z",
     "shell.execute_reply.started": "2024-02-21T03:58:31.543790Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T03:59:00.397199Z",
     "iopub.status.busy": "2024-02-21T03:59:00.396516Z",
     "iopub.status.idle": "2024-02-21T04:01:12.929545Z",
     "shell.execute_reply": "2024-02-21T04:01:12.928600Z",
     "shell.execute_reply.started": "2024-02-21T03:59:00.397164Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb6941a700d4870861a7e1b9b86e90d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#getting mistral from kaggle and initialize tokenizer\n",
    "model_name = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T04:01:31.589964Z",
     "iopub.status.busy": "2024-02-21T04:01:31.589597Z",
     "iopub.status.idle": "2024-02-21T04:01:31.845970Z",
     "shell.execute_reply": "2024-02-21T04:01:31.844966Z",
     "shell.execute_reply.started": "2024-02-21T04:01:31.589934Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import transformers\n",
    "\n",
    "text_generation_pipeline = transformers.pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    temperature=0.1,\n",
    "    do_sample=\"False\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    repetition_penalty=1.2,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=300,\n",
    ")\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T04:01:39.617242Z",
     "iopub.status.busy": "2024-02-21T04:01:39.616905Z",
     "iopub.status.idle": "2024-02-21T04:01:40.210796Z",
     "shell.execute_reply": "2024-02-21T04:01:40.209756Z",
     "shell.execute_reply.started": "2024-02-21T04:01:39.617217Z"
    }
   },
   "outputs": [],
   "source": [
    "#import dataset\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "file_path = '/kaggle/input/pokemon-corpus-2/corpus_df_2.csv'\n",
    "loader = CSVLoader(file_path=file_path)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T04:01:46.942694Z",
     "iopub.status.busy": "2024-02-21T04:01:46.941954Z",
     "iopub.status.idle": "2024-02-21T04:01:48.840397Z",
     "shell.execute_reply": "2024-02-21T04:01:48.839597Z",
     "shell.execute_reply.started": "2024-02-21T04:01:46.942660Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")\n",
    "\n",
    "#Create a split of the document using the text splitter\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T04:02:20.586147Z",
     "iopub.status.busy": "2024-02-21T04:02:20.585318Z",
     "iopub.status.idle": "2024-02-21T04:02:20.722122Z",
     "shell.execute_reply": "2024-02-21T04:02:20.721146Z",
     "shell.execute_reply.started": "2024-02-21T04:02:20.586111Z"
    }
   },
   "outputs": [],
   "source": [
    "#embedding model\n",
    "#voyage2\n",
    "from langchain_community.embeddings import VoyageEmbeddings\n",
    "\n",
    "embeddings = VoyageEmbeddings(model=\"voyage-2\",\n",
    "                              voyage_api_key=\"YOUR_API_KEY\",\n",
    "                              show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T04:02:46.154281Z",
     "iopub.status.busy": "2024-02-21T04:02:46.153371Z",
     "iopub.status.idle": "2024-02-21T04:59:23.510080Z",
     "shell.execute_reply": "2024-02-21T04:59:23.508948Z",
     "shell.execute_reply.started": "2024-02-21T04:02:46.154243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3427d8c8a5145babf7d61613f2fbb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1495 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T05:01:39.256214Z",
     "iopub.status.busy": "2024-02-21T05:01:39.255475Z",
     "iopub.status.idle": "2024-02-21T05:01:55.561754Z",
     "shell.execute_reply": "2024-02-21T05:01:55.560854Z",
     "shell.execute_reply.started": "2024-02-21T05:01:39.256173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42525e163e6c40b8a73d77fbe6741af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 242\n",
      "pokemon_info: Espeon (Japanese: エーフィ Eifie) is a Psychic-type Pokémon introduced in Generation II.\n",
      "It evolves from Eevee when leveled up with high friendship during the day.\n",
      "(Specifics may differ in past games. Refer to Game data→Evolution data for these details.)\n",
      "It is one of Eevee's final forms, the others being Vaporeon, Jolteon, Flareon, Umbreon, Leafeon, Glaceon, and Sylveon.\n",
      "Espeon is the starter Pokémon of Pokémon Colosseum alongside Umbreon.\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I evolve Eevee to Espeon?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:00:33.244048Z",
     "iopub.status.busy": "2024-02-21T06:00:33.243633Z",
     "iopub.status.idle": "2024-02-21T06:00:33.251629Z",
     "shell.execute_reply": "2024-02-21T06:00:33.250609Z",
     "shell.execute_reply.started": "2024-02-21T06:00:33.244017Z"
    }
   },
   "outputs": [],
   "source": [
    "#conversational chat history\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "#vectorstore = vectorstore\n",
    "retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs = {\"k\" : 5})\n",
    "\n",
    "# This controls how the standalone question is generated.\n",
    "# Should take `chat_history` and `question` as input variables.\n",
    "template = (\n",
    "    \"\"\"\n",
    "    <s>[INST]\n",
    "    You are an assisant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to \n",
    "    answer the question. Combine the chat history and\n",
    "    follow up question into a standalone question, if \n",
    "    the chat history is relevant to the question. If \n",
    "    you don't know the answer, say you don't know. \n",
    "    If the chat history is not relevant to the question,\n",
    "    do not focus on it.\n",
    "    </s>\n",
    "    [INST]\n",
    "    Chat History: {chat_history}\n",
    "    Follow up question: {question}\n",
    "    [/INST]\n",
    "    \"\"\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm = mistral_llm\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, retriever, prompt, return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:00:35.850453Z",
     "iopub.status.busy": "2024-02-21T06:00:35.849709Z",
     "iopub.status.idle": "2024-02-21T06:00:48.407466Z",
     "shell.execute_reply": "2024-02-21T06:00:48.406397Z",
     "shell.execute_reply.started": "2024-02-21T06:00:35.850424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa35a65382dc4d488896a613b3ae254c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, Earthquake is not good to use against Pidgeotto.\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"Is earthquake good to use against Pidgeotto?\"\n",
    "\n",
    "result = chain({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.append((question, result[\"answer\"]))\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:01:03.666305Z",
     "iopub.status.busy": "2024-02-21T06:01:03.665445Z",
     "iopub.status.idle": "2024-02-21T06:01:22.255492Z",
     "shell.execute_reply": "2024-02-21T06:01:22.254550Z",
     "shell.execute_reply.started": "2024-02-21T06:01:03.666272Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0158da136e304979860371a3b5db1217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "I think the best move against Pidgeotto would be Stone Edge.\n"
     ]
    }
   ],
   "source": [
    "question = \"What would be a good move against it?\"\n",
    "\n",
    "result = chain({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.append((question, result[\"answer\"]))\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:01:42.798741Z",
     "iopub.status.busy": "2024-02-21T06:01:42.797870Z",
     "iopub.status.idle": "2024-02-21T06:02:00.812151Z",
     "shell.execute_reply": "2024-02-21T06:02:00.811024Z",
     "shell.execute_reply.started": "2024-02-21T06:01:42.798704Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5eb3f4f3584c41b5dd2cb262fe46b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Garchomp is generally considered better for competitive battling due to its versatility and strong offensive capabilities.\n"
     ]
    }
   ],
   "source": [
    "question = \"Is Clefable or Garchomp better for competitive battling?\"\n",
    "\n",
    "result = chain({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.append((question, result[\"answer\"]))\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T06:02:06.877279Z",
     "iopub.status.busy": "2024-02-21T06:02:06.876352Z",
     "iopub.status.idle": "2024-02-21T06:02:30.983804Z",
     "shell.execute_reply": "2024-02-21T06:02:30.982847Z",
     "shell.execute_reply.started": "2024-02-21T06:02:06.877240Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b59c6b192ad4230801caccd1576950f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Both Clefable and Garchomp are excellent choices for defensive builds in competitive battling, each bringing unique strengths and weaknesses to the table. Clefable's decent bulk, reliable recovery, and fantastic defensive typing allow it to act as a solid check to Pokemon like Garchomp, Mega Medicham, Mega Lopunny, and Hawlucha, making it a valuable defensive Pokemon in the tier. On the other hand, Garchomp's powerful offensive capabilities and strong defensive typing make it a formidable opponent for any defensive build. Ultimately, the choice between these two will depend on your specific playstyle and preferences.\n"
     ]
    }
   ],
   "source": [
    "question = \"Which one is better for a defensive build?\"\n",
    "\n",
    "result = chain({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.append((question, result[\"answer\"]))\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4383783,
     "sourceId": 7526359,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
